{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def load_data(split):\n",
    "    df = pd.read_parquet(r'/home/mgteus/workspace/neuro/transformers_andrej/train_run.gzip')\n",
    "    feature_array = []\n",
    "    for x_pos,y_pos in zip(df['x_pos'], df['y_pos']):\n",
    "            feature_array.append(torch.tensor([x_pos, y_pos]))\n",
    "\n",
    "    feature_array = np.array(feature_array)\n",
    "    feature_array = torch.tensor(feature_array)\n",
    "    n = int(len(df)*0.8)\n",
    "    \n",
    "    return feature_array[:n] if split == 'train' else feature_array[n:]\n",
    "\n",
    "def get_batch(context_len, batch_size, split, device):\n",
    "    data = load_data(split=split)\n",
    "    ix = torch.randint(len(data) - context_len -1, (batch_size,))\n",
    "    x = torch.stack([data[i:i+context_len] for i in ix])\n",
    "    y = torch.stack([data[i+context_len+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def RMSELoss(pred, true):\n",
    "    criterion = nn.MSELoss()\n",
    "    return torch.sqrt(criterion(pred, true))\n",
    "\n",
    "\n",
    "\n",
    "class Head(nn.Module):\n",
    "    def __init__(self, context_len, batch_size, embed_size ,dropout) -> None:\n",
    "        super().__init__()\n",
    "        # parameters\n",
    "        self.batch_size = batch_size\n",
    "        self.context_len = context_len\n",
    "        self.dropout_value = dropout\n",
    "        self.embed_size = embed_size\n",
    "        # layers\n",
    "        #   # static layers\n",
    "        self.pos_to_enc_layer = nn.Linear(2, 2,)\n",
    "        self.enc_layer = nn.Linear(2, 1)\n",
    "        self.output_layer = nn.Linear(self.context_len, 2)\n",
    "        #   # dynamic layers\n",
    "        self.key = nn.Linear(self.embed_size, self.embed_size)\n",
    "        self.query = nn.Linear(self.embed_size, self.embed_size)\n",
    "        self.values = nn.Linear(self.embed_size, self.embed_size)\n",
    "\n",
    "        # tril\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(self.context_len, self.context_len)))\n",
    "\n",
    "        # dropout\n",
    "        self.dropout = nn.Dropout(self.dropout_value)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pos_to_enc_layer(x) # [x, y] -> [i, j]\n",
    "        x = self.enc_layer(x).squeeze(-1)        # [i, j] -> [k]\n",
    "\n",
    "        B, C = x.shape\n",
    "\n",
    "        k = self.key(x) # + x\n",
    "        q = self.query(x) # + x\n",
    "\n",
    "        wei = q @ k.transpose(-2, -1)  # [B, C] @ [C, B] -> [B, B]\n",
    "        #wei = wei.masked_fill(self.tril[:C, :C] == 0, float('-inf'))\n",
    "        wei = nn.functional.softmax(wei, dim=-1)\n",
    "\n",
    "        v = self.values(x)\n",
    "        out = wei @ v # [B, B] @ [B, C] -> [B, C]\n",
    "        \n",
    "        #out = self.output_layer(out) # [B, C] -> [B, 2]\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    CONTEXT_LEN = 10\n",
    "    BATCH_SIZE = 4\n",
    "    DROPOUT = 0.1\n",
    "    LEARNING_RATE = 1e-3\n",
    "    NUM_EPOCHS = 10\n",
    "\n",
    "    model = Head(context_len=CONTEXT_LEN, batch_size=BATCH_SIZE, dropout=DROPOUT)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    \n",
    "\n",
    "\n",
    "    loss_list = []\n",
    "\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        xb, yb = get_batch(context_len=CONTEXT_LEN, batch_size=BATCH_SIZE, split='train')\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        predictions = model(xb)\n",
    "        loss = RMSELoss(predictions, yb)\n",
    "        loss_list.append(loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(f\"iter. {epoch} - loss = {loss.item():4f}\")\n",
    "\n",
    "    plt.plot(loss_list)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
